{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_SUMMARIZATION = \"\"\"Please analyze and summarize the arxiv paper into an **Korean** AI newsletter. Feel free to include some technical keywords in English itself. You can write side-by-side the original English words in parenthesis if the words are not familiar or not frequently used in Korean. Please answer in JSON format where **keys are in English**. Consider the following format and components for the summary (but don't include all the keys if not applicable):\n",
    "[\n",
    "    {\"What's New\": \"...\"},\n",
    "    {\"Technical Details\": \"...\"},\n",
    "    {\"Performance Highlights\": \"...\"},\n",
    "]\n",
    "제발 부탁이야 한글로 요약해줘.\n",
    "\"\"\"\n",
    "\n",
    "paper_content = \"\"\"\n",
    "Title: Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models\n",
    "Abstract: This report introduces EEVE-Korean-v1.0, a Korean adaptation of large language models that exhibit remarkable capabilities across English and Korean text understanding. Building on recent highly capable but English-centric LLMs, such as SOLAR-10.7B and Phi-2, where non-English texts are inefficiently processed with English-centric tokenizers, we present an efficient and effective vocabulary expansion (EEVE) method, which encompasses parameter freezing and subword initialization. In contrast to previous efforts that believe new embeddings require trillions of training tokens, we show that our method can significantly boost non-English proficiency within just 2 billion tokens. Surpassing most instruction-tuned LLMs on the Open Ko-LLM Leaderboard, as of January 2024, our model EEVE-Korean-10.8B-v1.0 ranks as the leading Korean pre-trained model in the open-source community, according to Hugging Face’s leaderboard. We open-source our models on Huggingface to empower the open research community in various languages.\n",
    "\n",
    "Section 1: Introduction\n",
    "Recent advancements in the field of large language models (LLMs), such as GPT-4 OpenAI (2023), Gemini Team et al. (2023a), and Claude Anthropic (2023), have demonstrated remarkable capabilities in processing and understanding multiple languages. On the other hand, though notable models in open source community, such as LLaMA Touvron et al. (2023a, b), MPT Team et al. (2023b), Falcon Almazrouei et al. (2023), Mistral Jiang et al. (2023), Mixtral Jiang et al. (2024), SOLAR Kim et al. (2023), and Phi-1.5 Li et al. (2023b) have set benchmarks in English tasks, these developments have predominantly favored English, leading to a performance gap in non-English languages.\n",
    "\n",
    "Such disparity can be found not only in their language proficiency but also in computational efficiency, where non-English languages like Korean require significantly more tokens than English even for equivalent semantic content (Figure 1). And, of course, this negatively affects the user experiences, such as longer response times, shorter context lengths, and higher API costs Petrov et al. (2023). Expanding the tokenizer vocabulary, which introduces some frequently used yet long words as additional tokens, is thus indispensable for non-English users, but vocabulary expansion is a very challenging task because new embeddings require trillions of training tokens Zhao et al. (2024).\n",
    "\n",
    "To this end, this technical report presents a novel approach for efficient and effective vocabulary expansion, namely EEVE, which can better train the embeddings of newly added tokens. For ease of adaptation, we utilize subword-based embedding initialization and design seven training stages with parameter freezing, which elaborately adjust the order and amount of parameters to be trained. We meticulously transfer the advanced capabilities of foundational models from English to Korean by initially focusing on the training of only input embeddings and progressively expanding to encompass the full parameters in the final stage.\n",
    "\n",
    "Using EEVE, we officially release a family of Korean LLMs, EEVE-Korean-10.8B-v1.01\n",
    "1\n",
    "https://huggingface.co/yanolja/EEVE-Korean-10.8B-v1.0\n",
    " and EEVE-Korean-2.8B-v1.02\n",
    "2\n",
    "https://huggingface.co/yanolja/EEVE-Korean-2.8B-v1.0\n",
    ", which are built on recent English-centric LLMs, specifically SOLAR-10.7B Kim et al. (2023) and Phi-2 Li et al. (2023b), with further Korean-centric pre-training. We evaluate our models on lm-evaluation-harness3\n",
    "3\n",
    "https://github.com/EleutherAI/lm-evaluation-harness\n",
    " Gao et al. (2023) for both English and Korean language tasks, such as boolean question answering (BoolQ; Clark et al. 2019), commonsense causal reasoning (COPA; Roemmele et al. 2011, context-sensitive word understanding (WiC; Pilehvar and Camacho-Collados 2019), commonsense reasoning (HellaSwag; Zellers et al. 2019), and sentiment negation recognition (SentiNeg). From the evaluation, we observe that our models outperform the recent open Korean pre-trained LLMs like OPEN-SOLAR-KO-10.7B L. Junbum (2024), Polyglot-Ko Ko et al. (2023), and KoGPT Kim et al. (2021), while preserving the strong English capability of the base English-centric LLMs in terms of benchmark performance, being ranked as the leading Korean pre-trained model in Open Ko-LLM Leaderboard Park et al. (2023).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"What's New\": '다국어 대규모 언어 모델에 대한 효율적이고 효과적인 어휘 확장(EEVE)', 'Technical Details': 'EEVE 방법론은 매개변수 동결과 부분 단어 초기화를 통해, 새로운 임베딩이 수조 개의 훈련 토큰을 필요로 한다고 가정하는 기존 접근 방식들과는 대조적으로 한국어와 영어 텍스트 이해 능력에 있어 비영어 문서를 상당히 개선하였습니다. EEVE-Korean-10.8B-v1.0 및 EEVE-Korean-2.8B-v1.0과 같은 한국어 LLM 모델을 공개하고, Hugging Face의 리더보드에서 오픈소스 커뮤니티를 위한 선도적인 한국어 사전 훈련 모델로 평가받고 있습니다.', 'Performance Highlights': 'EEVE 모델들은 최근 한국어로 사전 훈련된 LLMs들인 OPEN-SOLEAR-KO-10.7B L. Junbum (2024), Polyglot-Ko Ko et al. (2023), 그리고 KoGPT Kim et al. (2021)를 능가하며, 벤치마크 성능은 영어 중심 LLMs의 우수한 영어 능력을 유지하여, Park et al. (2023)에 따른 오픈 코-LLM 리더보드에서 선도적인 한국어 사전 훈련 모델로 평가받았습니다.'}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# For Ollama, refer to https://www.youtube.com/watch?v=VkcaigvTrug\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"EEVE-Korean-10.8B:latest\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT_SUMMARIZATION},\n",
    "    {\"role\": \"user\", \"content\": paper_content},\n",
    "  ],\n",
    "  max_tokens=1024,\n",
    "  response_format={\n",
    "      \"type\": \"json_object\",\n",
    "  },\n",
    ")\n",
    "# print(response)\n",
    "print(json.loads(response.choices[0].message.content))\n",
    "{\"What's New\": '다국어 대규모 언어 모델에 대한 효율적이고 효과적인 어휘 확장(EEVE)', 'Technical Details': 'EEVE 방법론은 매개변수 동결과 부분 단어 초기화를 통해, 새로운 임베딩이 수조 개의 훈련 토큰을 필요로 한다고 가정하는 기존 접근 방식들과는 대조적으로 한국어와 영어 텍스트 이해 능력에 있어 비영어 문서를 상당히 개선하였습니다. EEVE-Korean-10.8B-v1.0 및 EEVE-Korean-2.8B-v1.0과 같은 한국어 LLM 모델을 공개하고, Hugging Face의 리더보드에서 오픈소스 커뮤니티를 위한 선도적인 한국어 사전 훈련 모델로 평가받고 있습니다.', 'Performance Highlights': 'EEVE 모델들은 최근 한국어로 사전 훈련된 LLMs들인 OPEN-SOLEAR-KO-10.7B L. Junbum (2024), Polyglot-Ko Ko et al. (2023), 그리고 KoGPT Kim et al. (2021)를 능가하며, 벤치마크 성능은 영어 중심 LLMs의 우수한 영어 능력을 유지하여, Park et al. (2023)에 따른 오픈 코-LLM 리더보드에서 선도적인 한국어 사전 훈련 모델로 평가받았습니다.'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
